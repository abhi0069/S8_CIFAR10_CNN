# -*- coding: utf-8 -*-
"""S8_Model_8(albumentations) copy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C8lhSgxGys22moDvr5IYoHGndrJrbYxg

# Import Libraries
"""

from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

import numpy as np
import matplotlib.pyplot as plt

#!pip install albumentations

import albumentations as A
from albumentations.pytorch import ToTensorV2

from multiprocessing import freeze_support
from tqdm import tqdm

# Constants
CIFAR_MEAN = (0.4914, 0.4822, 0.4465)
CIFAR_STD = (0.2470, 0.2435, 0.2616)

# Define transforms
train_transforms = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),
    A.CoarseDropout(
        max_holes=1, max_height=16, max_width=16,
        min_holes=1, min_height=16, min_width=16,
        fill_value=CIFAR_MEAN, mask_fill_value=None,
        p=0.5
    ),
    A.Normalize(mean=CIFAR_MEAN, std=CIFAR_STD),
    ToTensorV2()
])

test_transforms = A.Compose([
    A.Normalize(mean=CIFAR_MEAN, std=CIFAR_STD),
    ToTensorV2()
])

# Define AlbumentationDataset class
class AlbumentationDataset:
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform
        
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        if idx == 0:
            print(f"Original image type: {type(image)}")
        
        image = np.array(image)
        
        if self.transform:
            try:
                transformed = self.transform(image=image)
                image = transformed["image"]
            except Exception as e:
                print(f"Transform error: {e}")
                print(f"Image shape: {image.shape}")
                raise e
        
        return image, label

# Define model classes
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,
                                  stride=stride, padding=padding, groups=in_channels)
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # C1 Block - Initial features with dilation=1
        self.convblock1 = nn.Sequential(
            nn.Conv2d(3, 24, 3, padding=1, dilation=1, bias=False),  # RF: 3
            nn.ReLU(),
            nn.BatchNorm2d(24),
            nn.Conv2d(24, 24, 3, padding=1, dilation=1, bias=False),  # RF: 5
            nn.ReLU(),
            nn.BatchNorm2d(24)
        )
        
        # C2 Block with Depthwise Separable Conv
        self.convblock2 = nn.Sequential(
            DepthwiseSeparableConv(24, 32, 3, padding=2),  # RF: 9
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.Conv2d(32, 32, 3, padding=2, dilation=2, bias=False),  # RF: 13
            nn.ReLU(),
            nn.BatchNorm2d(32)
        )
        
        # C3 Block with dilation=4
        self.convblock3 = nn.Sequential(
            nn.Conv2d(32, 48, 3, padding=4, dilation=4, bias=False),  # RF: 25
            nn.ReLU(),
            nn.BatchNorm2d(48),
            nn.Conv2d(48, 48, 3, padding=4, dilation=4, bias=False),  # RF: 41
            nn.ReLU(),
            nn.BatchNorm2d(48)
        )
        
        # C4 Block with dilation=8
        self.convblock4 = nn.Sequential(
            nn.Conv2d(48, 64, 3, padding=8, dilation=8, bias=False),  # RF: 73
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.Conv2d(64, 64, 3, padding=8, dilation=8, bias=False),  # RF: 105
            nn.ReLU(),
            nn.BatchNorm2d(64)
        )
        
        # GAP + Dropout + FC
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(64, 10)

    def forward(self, x):
        x = self.convblock1(x)
        x = self.convblock2(x)
        x = self.convblock3(x)
        x = self.convblock4(x)
        x = self.gap(x)
        x = x.view(-1, 64)
        x = self.dropout(x)
        x = self.fc(x)
        return F.log_softmax(x, dim=1)

# Move these before if __name__ == '__main__':
train_losses = []
test_losses = []
train_acc = []
test_acc = []

def train(model, device, train_loader, optimizer, scheduler, epoch):
    model.train()
    pbar = tqdm(train_loader, position=0, leave=True)
    correct = 0
    processed = 0
    running_loss = 0.0  # Track average loss for the epoch
    
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        y_pred = model(data)
        loss = F.nll_loss(y_pred, target)
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        # Update running loss
        running_loss += loss.item()
        
        pred = y_pred.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        processed += len(data)
        
        pbar.set_description(
            f'Epoch: {epoch} Loss={loss.item():.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:.2f}'
        )
    
    # Store epoch metrics
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / processed
    train_acc.append(epoch_acc)
    train_losses.append(epoch_loss)

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    test_losses.append(test_loss)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))

    test_acc.append(100. * correct / len(test_loader.dataset))

# Then the main block
if __name__ == '__main__':
    freeze_support()
    
    # CUDA setup
    SEED = 1
    cuda = torch.cuda.is_available()
    print("CUDA Available?", cuda)
    device = torch.device("cuda" if cuda else "cpu")
    
    # For reproducibility
    torch.manual_seed(SEED)
    if cuda:
        torch.cuda.manual_seed(SEED)
    
    # Create datasets and dataloaders
    train_data = datasets.CIFAR10('./data', train=True, download=True, transform=None)
    test_data = datasets.CIFAR10('./data', train=False, download=True, transform=None)
    
    train_dataset = AlbumentationDataset(train_data, train_transforms)
    test_dataset = AlbumentationDataset(test_data, test_transforms)
    
    dataloader_args = dict(shuffle=True, batch_size=64, num_workers=0, pin_memory=True) if cuda else dict(shuffle=True, batch_size=32)
    train_loader = torch.utils.data.DataLoader(train_dataset, **dataloader_args)
    test_loader = torch.utils.data.DataLoader(test_dataset, **dataloader_args)
    
    # Create model and move to device
    model = Net().to(device)
    
    # Setup optimizer and scheduler
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=0.1,
        epochs=30,
        steps_per_epoch=len(train_loader),
        pct_start=0.2,
        div_factor=10
    )
    
    # Training loop
    EPOCHS = 30
    for epoch in range(EPOCHS):
        print("EPOCH:", epoch)
        train(model, device, train_loader, optimizer, scheduler, epoch)
        test(model, device, test_loader)
    
    # Plot results
    fig, axs = plt.subplots(2,2,figsize=(15,10))
    axs[0, 0].plot(train_losses)
    axs[0, 0].set_title("Training Loss")
    axs[1, 0].plot(train_acc)
    axs[1, 0].set_title("Training Accuracy")
    axs[0, 1].plot(test_losses)
    axs[0, 1].set_title("Test Loss")
    axs[1, 1].plot(test_acc)
    axs[1, 1].set_title("Test Accuracy")
    plt.show()

"""## Data Statistics"""
# Use raw data for statistics
raw_train_data = train_data.data  # Already numpy array
print('[Train]')
print(' - Numpy Shape:', raw_train_data.shape)
print(' - Tensor Shape:', torch.tensor(raw_train_data).size())
print(' - min:', torch.min(torch.tensor(raw_train_data)))
print(' - max:', torch.max(torch.tensor(raw_train_data)))
print(' - mean:', torch.mean(torch.tensor(raw_train_data).float()))
print(' - std:', torch.std(torch.tensor(raw_train_data).float()))
print(' - var:', torch.var(torch.tensor(raw_train_data).float()))

"""## MORE

It is important that we view as many images as possible. This is required to get some idea on image augmentation later on
"""
'''
figure = plt.figure(figsize=(10,8))
num_of_images = 20
for index in range(1, num_of_images + 1):
    plt.subplot(4, 5, index)
    plt.axis('off')
    plt.imshow(images[index].permute(1, 2, 0).cpu().numpy())
plt.show()
'''
"""# The model
Let's start with the model we first saw
"""

#!pip install torchsummary
from torchsummary import summary
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
print(device)
model = Net().to(device)
summary(model, input_size=(3, 32, 32))

